{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46793ada",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Movie Review Data with Logistic Regression and Naive Bayes Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081faea8",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b9bc2",
   "metadata": {},
   "source": [
    "The goal of this project is to perform a sentiment analysis of text data from movie reviews using two different classification algorithms - Logistic Regression and Naive Bayes - and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31e156f",
   "metadata": {},
   "source": [
    "## Data Collection and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b422e9",
   "metadata": {},
   "source": [
    "This dataset was organized and labeled by Bo Pang and Lillian Li, and used in their paper \"Seeing Stars: Exploiting class relationships for sentiment categorization with respect to rating scales\" (2005) from Proceedings of the ACL. It contains 5331 positive and 5331 negative review snippets culled from the Rotten Tomatoes movie review website. Positive reviews were those designated as \"fresh\" and negative reviews were those designated \"rotten\" on the website. The full data set can be accessed [here](http://www.cs.cornell.edu/people/pabo/movie-review-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3378ed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Positive Reviews: \n",
      "['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .', 'effective but too-tepid biopic']\n",
      "Sample Negative Reviews: \n",
      "['simplistic , silly and tedious .', \"it's so laddish and juvenile , only teenage boys could possibly find it funny .\", 'exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .']\n"
     ]
    }
   ],
   "source": [
    "# initialize empty lists to store positive and negative reviews\n",
    "positive_reviews = []\n",
    "negative_reviews = []\n",
    "\n",
    "# read the positive and negative reviews into their respective lists from the data files\n",
    "with open(\"positive_reviews.txt\", \"r\") as positive_file:\n",
    "    for line in positive_file:\n",
    "        # remove leading/trailing whitespaces and add to positive_reviews\n",
    "        positive_reviews.append(line.strip())\n",
    "\n",
    "with open(\"negative_reviews.txt\", \"r\") as negative_file:\n",
    "    for line in negative_file:\n",
    "        # remove leading/trailing whitespaces and add to positive_reviews\n",
    "        negative_reviews.append(line.strip())\n",
    "        \n",
    "# print the first few lines from each list as a sample\n",
    "print(\"Sample Positive Reviews: \")\n",
    "print(positive_reviews[:3])\n",
    "print(\"Sample Negative Reviews: \")\n",
    "print(negative_reviews[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5332e87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10662\n",
      "10662\n"
     ]
    }
   ],
   "source": [
    "# create the x-variable\n",
    "reviews = positive_reviews + negative_reviews\n",
    "print(len(reviews))\n",
    "# create the y-variable\n",
    "labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850bd498",
   "metadata": {},
   "source": [
    "clean and tokenize the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7cb32e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vscerra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "nltk.download('punkt') # pre-trained tokenizer Punkt for English\n",
    "def tokenize(corpus):\n",
    "    data = []\n",
    "    for text in corpus: \n",
    "        text = re.sub(r'\\\\', ' ', text)\n",
    "        text = re.sub(r'[,!?;-]+', '.', text)\n",
    "        words = nltk.word_tokenize(text) #tokenize strings to words\n",
    "        words = [word.lower() for word in words\n",
    "                if word.isalpha()\n",
    "                or word == '.'\n",
    "               ]\n",
    "        data.extend(words)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1384820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .', 'effective but too-tepid biopic', 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .', \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\", 'the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .', 'offers that rare combination of entertainment and education .', 'perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .', \"steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\", 'take care of my cat offers a refreshingly different slice of asian cinema .']\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d574822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenize(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f66baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of words: ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', 'century', 'new', 'conan', 'and', 'that', 'he', 'going', 'to', 'make', 'a', 'splash', 'even', 'greater']\n"
     ]
    }
   ],
   "source": [
    "print(f'Sample of words: {data[:20]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241ec8e",
   "metadata": {},
   "source": [
    "now make a sliding window of words - Slide a window of words across the list. For each window, we extract the center word and surrounding context words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b996feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i-C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e3c0df",
   "metadata": {},
   "source": [
    "The first argument in the `get_windows` function is the list of tokenized words, and the second argument is the context half-size. For a given center word, the context words are the `C` flanking words on either side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b279b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now', 'the']\ttry\n",
      "['try', 'tokenizer']\tthe\n",
      "['the', 'with']\ttokenizer\n",
      "['tokenizer', 'a']\twith\n",
      "['with', 'new']\ta\n",
      "['a', 'sentence']\tnew\n"
     ]
    }
   ],
   "source": [
    "for x, y in get_windows(tokenize([\"Now try the tokenizer with a new sentence\"]), 1):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6daa19",
   "metadata": {},
   "source": [
    "### Transform the training set reviews into vectors\n",
    "\n",
    "We want to transfer the center words into one-hot vectors, and the context words into averages of one-hot vectors. \n",
    "\n",
    "To create the one-hot vectors, start by mapping each word to a unique index. The function below creates Python dictionaries that map words to integers (word2ind) and back (ind2word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2529fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_ind_mapping(words):\n",
    "    # create a set of unique words in the list\n",
    "    unique_words = list(set(words))\n",
    "    \n",
    "    # create a word to index mapping (word2Ind)\n",
    "    word2ind = {word: idx for idx, word in enumerate(unique_words)}\n",
    "    \n",
    "    # create an index to word mapping (Ind2word)\n",
    "    ind2word = {idx: word for idx, word in enumerate(unique_words)}\n",
    "    \n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a776a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabluary:  17596\n"
     ]
    }
   ],
   "source": [
    "word2ind, ind2word = word_to_ind_mapping(data)\n",
    "V = len(word2ind)\n",
    "print(\"size of vocabluary: \", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f8b5d0",
   "metadata": {},
   "source": [
    "### Get one-hot word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e106fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_one_hot_vector(word, word2ind, V):\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2ind[word]] = 1\n",
    "    \n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda37018",
   "metadata": {},
   "source": [
    "Getting the average of one-hot vectors for context words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e17dcca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_words_to_vector(context_words, word2ind, V):\n",
    "    context_word_vectors = [word_to_one_hot_vector(w, word2ind, V) for w in context_words]\n",
    "    context_word_vectors = np.mean(context_word_vectors, axis = 0)\n",
    "    \n",
    "    return context_word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c25bc",
   "metadata": {},
   "source": [
    "## Train the CBOW model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e1fe85",
   "metadata": {},
   "source": [
    "### Initialize the model matrices and vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b153aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N, V, random_seed = 1):\n",
    "    '''\n",
    "    Inputs: \n",
    "        N:  dimension of hidden vector \n",
    "        V:  dimension of vocabulary\n",
    "        random_seed: random seed for consistent results during testbedding\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2: initialized weights and biases\n",
    "    '''\n",
    "    np.random.seed(random_seed)\n",
    "    # W1 has shape (N,V)\n",
    "    W1 = np.random.rand(N,V)\n",
    "    \n",
    "    # W2 has shape (V,N)\n",
    "    W2 = np.random.rand(V,N)\n",
    "    \n",
    "    # b1 has shape (N,1)\n",
    "    b1 = np.random.rand(N,1)\n",
    "    \n",
    "    # b2 has shape (V,1)\n",
    "    b2 = np.random.rand(V,1)\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3008059e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_W1.shape: (4, 10)\n",
      "tmp_W2.shape: (10, 4)\n",
      "tmp_b1.shape: (4, 1)\n",
      "tmp_b2.shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test your function example.\n",
    "tmp_N = 4\n",
    "tmp_V = 10\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "assert tmp_W1.shape == ((tmp_N,tmp_V))\n",
    "assert tmp_W2.shape == ((tmp_V,tmp_N))\n",
    "print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape: {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape: {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape: {tmp_b2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c3d02d",
   "metadata": {},
   "source": [
    "### Implement the softmax and ReLU activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b074bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z)\n",
    "    \n",
    "    return e_z / sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac4d361d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02917235, 0.01073191, 0.21555612, 0.58594229, 0.07929867,\n",
       "       0.07929867])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the softmax function\n",
    "softmax([9, 8, 11, 12, 10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c041cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z): \n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c4c6dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  3,  0, 10]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the relu function\n",
    "relu(np.array([[-1, 2, 3, -100, 10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cdfb9a",
   "metadata": {},
   "source": [
    "### Implement forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1506703",
   "metadata": {},
   "source": [
    "Implement the forward propagation of $z$ according to the following equations. <br>\n",
    "\n",
    "\\begin{align}\n",
    " h &= W_1 \\  X + b_1   \\\\\n",
    " h &= ReLU(h)  \\\\\n",
    " z &= W_2 \\  h + b_2    \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2cb5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, W1, W2, b1, b2, relu = relu):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        W1, W2, b1, b2:  matrices and biases to be learned\n",
    "     Outputs: \n",
    "        z:  output score vector\n",
    "    '''\n",
    "    # calculate h\n",
    "    z1 = np.dot(W1,x) + b1\n",
    "    \n",
    "    # apply relu on h, store in h\n",
    "    h = relu(z1)\n",
    "    \n",
    "    # calculate z\n",
    "    z = np.dot(W2, h) + b2\n",
    "    \n",
    "    return z, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "421ed5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x has shape (3, 1)\n",
      "N is 2 and vocabulary size V is 3\n",
      "call forward_prop\n",
      "\n",
      "z has shape (3, 1)\n",
      "z has values:\n",
      "[[0.55379268]\n",
      " [1.58960774]\n",
      " [1.50722933]]\n",
      "\n",
      "h has shape (2, 1)\n",
      "h has values:\n",
      "[[0.92477674]\n",
      " [1.02487333]]\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "\n",
    "# Create some inputs\n",
    "tmp_N = 2\n",
    "tmp_V = 3\n",
    "tmp_x = np.array([[0,1,0]]).T\n",
    "#print(tmp_x)\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N,V=tmp_V, random_seed=1)\n",
    "\n",
    "print(f\"x has shape {tmp_x.shape}\")\n",
    "print(f\"N is {tmp_N} and vocabulary size V is {tmp_V}\")\n",
    "\n",
    "# call function\n",
    "tmp_z, tmp_h = forward_propagation(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(\"call forward_prop\")\n",
    "print()\n",
    "# Look at output\n",
    "print(f\"z has shape {tmp_z.shape}\")\n",
    "print(\"z has values:\")\n",
    "print(tmp_z)\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"h has shape {tmp_h.shape}\")\n",
    "print(\"h has values:\")\n",
    "print(tmp_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a2380e",
   "metadata": {},
   "source": [
    "### Generate data batches to run through the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b923ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(corpus, word2ind, V, C, batch_size):\n",
    "    data = [word2ind[word] for word in corpus if word in word2ind]\n",
    "    \n",
    "    context_word_matrix_list = []  # Initialize a list to store context word matrices\n",
    "    center_word_matrix_list = []  # Initialize a list to store center word matrices\n",
    "        \n",
    "    for i in range(C, len(data) - C):\n",
    "        context_words = []\n",
    "        center_word = data[i]\n",
    "        \n",
    "        # collect context words in the window\n",
    "        for j in range(i-C, i):\n",
    "            context_words.append(data[j])\n",
    "        for j in range(i+1, i+C+1):\n",
    "            context_words.append(data[j])\n",
    "            \n",
    "        # convert context and center words to vectors\n",
    "        context_vector = context_words_to_vector([corpus[idx] for idx in context_words], word2ind, V)\n",
    "        center_word_vector = word_to_one_hot_vector(corpus[center_word], word2ind, V)\n",
    "        \n",
    "        # append context vector and center vector to their respective lists\n",
    "        context_word_matrix_list.append(context_vector)\n",
    "        center_word_matrix_list.append(center_word_vector)\n",
    "        \n",
    "        # if the batch size is reached, yield the batch and reset it\n",
    "        if len(context_word_matrix_list) == batch_size:\n",
    "            yield np.column_stack(context_word_matrix_list), np.column_stack(center_word_matrix_list)\n",
    "            # reset the matrix lists\n",
    "            context_word_matrix_list = []\n",
    "            center_word_matrix_list = []\n",
    "            \n",
    "    # yield any remaining batches\n",
    "    if context_word_matrix_list: \n",
    "        yield  np.column_stack(context_word_matrix_list), np.column_stack(center_word_matrix_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0874535",
   "metadata": {},
   "source": [
    "### Implement the cross-entropy cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e11530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y, yhat, batch_size): \n",
    "    # cost function\n",
    "    logprobs = np.multiply(np.log(yhat),y)\n",
    "    cost = -1 / batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd5e1332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_x.shape (17596, 4)\n",
      "tmp_y.shape (17596, 4)\n",
      "tmp_W1.shape (50, 17596)\n",
      "tmp_W2.shape (17596, 50)\n",
      "tmp_b1.shape (50, 1)\n",
      "tmp_b2.shape (17596, 1)\n",
      "tmp_z.shape: (17596, 4)\n",
      "tmp_h.shape: (50, 4)\n",
      "tmp_yhat.shape: (17596, 4)\n",
      "call compute_cost\n",
      "tmp_cost 14.0589\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "tmp_C = 2\n",
    "tmp_N = 50\n",
    "tmp_batch_size = 4\n",
    "tmp_word2Ind, tmp_Ind2word = word_to_ind_mapping(data)\n",
    "tmp_V = len(word2ind)\n",
    "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
    "        \n",
    "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
    "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
    "\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "\n",
    "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "\n",
    "tmp_z, tmp_h = forward_propagation(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
    "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
    "\n",
    "tmp_yhat = softmax(tmp_z)\n",
    "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
    "\n",
    "tmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\n",
    "print(\"call compute_cost\")\n",
    "print(f\"tmp_cost {tmp_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db0972",
   "metadata": {},
   "source": [
    "### Implement backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7092645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        yhat: prediction (estimate of y)\n",
    "        y:  target vector\n",
    "        h:  hidden vector\n",
    "        W1, W2, b1, b2:  matrices and biases  \n",
    "        batch_size: batch size \n",
    "     Outputs: \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
    "    '''\n",
    "        \n",
    "    # compute z1 as W1x + b1\n",
    "    z1 = np.dot(W1,x) +  b1\n",
    "    \n",
    "    # initialize gradients\n",
    "    grad_W1 = np.zeros_like(W1)\n",
    "    grad_W2 = np.zeros_like(W2)\n",
    "    grad_b1 = np.zeros_like(b1)\n",
    "    grad_b2 = np.zeros_like(b2)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x_i = x[:, i].reshape(-1, 1)\n",
    "        y_i = y[:, i].reshape(-1, 1)\n",
    "        yhat_i = yhat[:, i].reshape(-1,1)\n",
    "        z1_i = z1[:, i].reshape(-1, 1)\n",
    "        \n",
    "        # compute l1 as W2^T (yhat - y)\n",
    "        l1 = np.dot(W2.T, yhat_i - y_i)\n",
    "        l1[z1_i < 0] = 0\n",
    "        \n",
    "        # use l1 to compute gradients below\n",
    "        \n",
    "        # compute gradient for W1\n",
    "        grad_W1 += np.outer(l1, x_i)\n",
    "        \n",
    "        # compute gradient for W2\n",
    "        grad_W2 += np.outer(yhat_i - y_i, h[:, i].reshape(-1,1))\n",
    "        \n",
    "        # compute gradient for b1\n",
    "        grad_b1 += l1\n",
    "        \n",
    "        # compute graident for b2 \n",
    "        grad_b2 += (yhat_i - y_i)\n",
    "        \n",
    "    # average the gradients over the batch\n",
    "    grad_W1 /= batch_size\n",
    "    grad_W2 /= batch_size\n",
    "    grad_b1 /= batch_size\n",
    "    grad_b2 /= batch_size\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee0a8319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get a batch of data\n",
      "tmp_x.shape (17596, 4)\n",
      "tmp_y.shape (17596, 4)\n",
      "\n",
      "Initialize weights and biases\n",
      "tmp_W1.shape (50, 17596)\n",
      "tmp_W2.shape (17596, 50)\n",
      "tmp_b1.shape (50, 1)\n",
      "tmp_b2.shape (17596, 1)\n",
      "\n",
      "Forwad prop to get z and h\n",
      "tmp_z.shape: (17596, 4)\n",
      "tmp_h.shape: (50, 4)\n",
      "\n",
      "Get yhat by calling softmax\n",
      "tmp_yhat.shape: (17596, 4)\n",
      "\n",
      "call back_prop\n",
      "tmp_grad_W1.shape (50, 17596)\n",
      "tmp_grad_W2.shape (17596, 50)\n",
      "tmp_grad_b1.shape (50, 1)\n",
      "tmp_grad_b2.shape (17596, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "tmp_C = 2\n",
    "tmp_N = 50\n",
    "tmp_batch_size = 4\n",
    "tmp_word2Ind, tmp_Ind2word = word_to_ind_mapping(data)\n",
    "tmp_V = len(word2ind)\n",
    "\n",
    "\n",
    "# get a batch of data\n",
    "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
    "\n",
    "print(\"get a batch of data\")\n",
    "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
    "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Initialize weights and biases\")\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "\n",
    "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Forwad prop to get z and h\")\n",
    "tmp_z, tmp_h = forward_propagation(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
    "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Get yhat by calling softmax\")\n",
    "tmp_yhat = softmax(tmp_z)\n",
    "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
    "\n",
    "tmp_m = (2*tmp_C)\n",
    "tmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_propagation(tmp_x, tmp_yhat, tmp_y, tmp_h, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n",
    "\n",
    "print()\n",
    "print(\"call back_prop\")\n",
    "print(f\"tmp_grad_W1.shape {tmp_grad_W1.shape}\")\n",
    "print(f\"tmp_grad_W2.shape {tmp_grad_W2.shape}\")\n",
    "print(f\"tmp_grad_b1.shape {tmp_grad_b1.shape}\")\n",
    "print(f\"tmp_grad_b2.shape {tmp_grad_b2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb7b65",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9ec39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data, word2ind, N, V, C, num_iters, alpha = 0.01,\n",
    "                    random_seed = 1, initialize_model = initialize_model, \n",
    "                    get_batches = get_batches, forward_propagation = forward_propagation,\n",
    "                    softmax = softmax, compute_cost = compute_cost,\n",
    "                    back_propagation = back_propagation): \n",
    "    '''\n",
    "      Inputs: \n",
    "        data:      text\n",
    "        word2ind:  words to Indices\n",
    "        N: dimension of hidden vector  \n",
    "        V: dimension of vocabulary \n",
    "        C: size of the context around center word\n",
    "        num_iters: number of iterations  \n",
    "        random_seed: random seed to initialize the model's matrices and vectors\n",
    "        initialize_model: your implementation of the function to initialize the model\n",
    "        get_batches: function to get the data in batches\n",
    "        forward_propagation: your implementation of the function to perform forward propagation\n",
    "        softmax: your implementation of the softmax function\n",
    "        compute_cost: cost function (Cross entropy)\n",
    "        back_propagation: your implementation of the function to perform backward propagation\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases after num_iters iterations\n",
    "\n",
    "    '''\n",
    "    W1, W2, b1, b2 = initialize_model(N, V, random_seed = random_seed) \n",
    "    \n",
    "    batch_size = 36\n",
    "    iters = 0\n",
    "    \n",
    "    for x, y in get_batches(data, word2ind, V, C, batch_size):\n",
    "        # get z and h\n",
    "        z, h = forward_propagation(x, W1, W2, b1, b2)\n",
    "        \n",
    "        # get yhat\n",
    "        yhat = softmax(z)\n",
    "        \n",
    "        # get cost\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        if ((iters + 1) % 10 == 0):\n",
    "            print(f\"iters: {iters + 1} cost: {cost: .6f}\")\n",
    "            \n",
    "        # get gradients \n",
    "        #print(x.shape, yhat.shape, y.shape, h.shape, W1.shape, W2.shape, b1.shape, b2.shape, batch_size)\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_propagation(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "        \n",
    "        # update weights and biases\n",
    "        W1 = W1 - alpha * grad_W1\n",
    "        W2 = W2 - alpha * grad_W2\n",
    "        b1 = b1 - alpha * grad_b1\n",
    "        b2 = b2 - alpha * grad_b2\n",
    "        \n",
    "        iters += 1\n",
    "        if iters == num_iters:\n",
    "            break\n",
    "        if iters % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "        \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f1085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 10 cost:  20.926946\n",
      "iters: 20 cost:  20.431572\n",
      "iters: 30 cost:  19.499856\n",
      "iters: 40 cost:  19.447751\n",
      "iters: 50 cost:  24.913858\n",
      "iters: 60 cost:  30.860422\n",
      "iters: 70 cost:  41.432009\n",
      "iters: 80 cost:  52.029636\n",
      "iters: 90 cost:  61.915792\n",
      "iters: 100 cost:  67.170580\n",
      "iters: 110 cost:  74.411181\n",
      "iters: 120 cost:  88.073722\n",
      "iters: 130 cost:  75.784061\n",
      "iters: 140 cost:  114.903931\n",
      "iters: 150 cost:  111.851899\n",
      "iters: 160 cost:  95.805811\n",
      "iters: 170 cost:  131.184741\n",
      "iters: 180 cost:  141.219405\n",
      "iters: 190 cost:  170.470644\n",
      "iters: 200 cost:  170.023310\n",
      "iters: 210 cost:  166.134576\n",
      "iters: 220 cost:  168.716066\n",
      "iters: 230 cost:  202.762959\n",
      "iters: 240 cost:  201.496028\n",
      "iters: 250 cost:  197.613939\n",
      "iters: 260 cost:  184.757066\n",
      "iters: 270 cost:  244.970770\n",
      "iters: 280 cost:  218.092769\n",
      "iters: 290 cost:  217.142600\n",
      "iters: 300 cost:  228.613582\n",
      "iters: 310 cost:  296.191329\n",
      "iters: 320 cost:  270.879330\n",
      "iters: 330 cost:  260.124359\n",
      "iters: 340 cost:  275.910939\n",
      "iters: 350 cost:  304.373955\n",
      "iters: 360 cost:  330.960327\n",
      "iters: 370 cost:  320.343479\n",
      "iters: 380 cost:  366.493059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vscerra\\AppData\\Local\\Temp\\ipykernel_19736\\608088107.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  e_z = np.exp(z)\n",
      "C:\\Users\\vscerra\\AppData\\Local\\Temp\\ipykernel_19736\\608088107.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return e_z / sum_e_z\n",
      "C:\\Users\\vscerra\\AppData\\Local\\Temp\\ipykernel_19736\\119996856.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  logprobs = np.multiply(np.log(yhat),y)\n",
      "C:\\Users\\vscerra\\AppData\\Local\\Temp\\ipykernel_19736\\119996856.py:3: RuntimeWarning: invalid value encountered in multiply\n",
      "  logprobs = np.multiply(np.log(yhat),y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters: 390 cost:  nan\n",
      "iters: 400 cost:  nan\n",
      "iters: 410 cost:  nan\n",
      "iters: 420 cost:  nan\n",
      "iters: 430 cost:  nan\n",
      "iters: 440 cost:  nan\n",
      "iters: 450 cost:  nan\n",
      "iters: 460 cost:  nan\n",
      "iters: 470 cost:  nan\n",
      "iters: 480 cost:  nan\n",
      "iters: 490 cost:  nan\n",
      "iters: 500 cost:  nan\n",
      "iters: 510 cost:  nan\n",
      "iters: 520 cost:  nan\n",
      "iters: 530 cost:  nan\n",
      "iters: 540 cost:  nan\n",
      "iters: 550 cost:  nan\n",
      "iters: 560 cost:  nan\n",
      "iters: 570 cost:  nan\n",
      "iters: 580 cost:  nan\n",
      "iters: 590 cost:  nan\n",
      "iters: 600 cost:  nan\n",
      "iters: 610 cost:  nan\n",
      "iters: 620 cost:  nan\n",
      "iters: 630 cost:  nan\n",
      "iters: 640 cost:  nan\n",
      "iters: 650 cost:  nan\n",
      "iters: 660 cost:  nan\n",
      "iters: 670 cost:  nan\n",
      "iters: 680 cost:  nan\n",
      "iters: 690 cost:  nan\n",
      "iters: 700 cost:  nan\n",
      "iters: 710 cost:  nan\n",
      "iters: 720 cost:  nan\n",
      "iters: 730 cost:  nan\n",
      "iters: 740 cost:  nan\n",
      "iters: 750 cost:  nan\n",
      "iters: 760 cost:  nan\n",
      "iters: 770 cost:  nan\n",
      "iters: 780 cost:  nan\n",
      "iters: 790 cost:  nan\n",
      "iters: 800 cost:  nan\n",
      "iters: 810 cost:  nan\n",
      "iters: 820 cost:  nan\n",
      "iters: 830 cost:  nan\n",
      "iters: 840 cost:  nan\n",
      "iters: 850 cost:  nan\n",
      "iters: 860 cost:  nan\n",
      "iters: 870 cost:  nan\n",
      "iters: 880 cost:  nan\n",
      "iters: 890 cost:  nan\n",
      "iters: 900 cost:  nan\n",
      "iters: 910 cost:  nan\n",
      "iters: 920 cost:  nan\n",
      "iters: 930 cost:  nan\n",
      "iters: 940 cost:  nan\n",
      "iters: 950 cost:  nan\n",
      "iters: 960 cost:  nan\n",
      "iters: 970 cost:  nan\n",
      "iters: 980 cost:  nan\n",
      "iters: 990 cost:  nan\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "C = 2\n",
    "N = 200\n",
    "word2Ind, Ind2word = word_to_ind_mapping(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 1000\n",
    "print(\"Call gradient_descent\")\n",
    "W1, W2, b1, b2 = gradient_descent(data, word2ind, N, V, C, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7cc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854e669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f33dd25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a408ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7b536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "290d2790",
   "metadata": {},
   "source": [
    "#### Cost function and gradient\n",
    "\n",
    "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) $$\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of training example 'i'.\n",
    "* $h(z^{(i)})$ is the model's prediction for the training example 'i'.\n",
    "\n",
    "The loss function for a single training example is\n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
